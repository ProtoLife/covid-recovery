{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Jupyter Specifics\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets.widgets import interact, interactive, IntSlider, FloatSlider, Layout, ToggleButton, ToggleButtons, fixed\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "style = {'description_width': '100px'}\n",
    "slider_layout = Layout(width='99%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(countries_jhu_4_owid),len(countries_jhu_2_owid),len(countries_owid),len(countries_jhu))\n",
    "print('countries in common: owid format')\n",
    "print(countries_jhu_2_owid)\n",
    "print('')\n",
    "print('owid countries not in common set')\n",
    "print(set(countries_owid)-set(countries_jhu_2_owid))\n",
    "print('')\n",
    "print('countries in common: jhu format')\n",
    "print(countries_owid_to_jhu)\n",
    "print('')\n",
    "print(len(bcountries),'bcountries',bcountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases = [c for c in clusdata_all]\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = ['deaths','cases','cases_lin2020','cases_pwlfit','cases_nonlin']\n",
    "\n",
    "d_countries = [c for c in clusdata_all['deaths']]\n",
    "c_countries = [c for c in clusdata_all['cases']]\n",
    "lc_countries = [c for c in clusdata_all['cases_lin2020']]\n",
    "pc_countries = [c for c in clusdata_all['cases_pwlfit']]\n",
    "nc_countries = [c for c in clusdata_all['cases_nonlin']]\n",
    "\n",
    "countries = d_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(d_countries))\n",
    "print(np.sort(d_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that all country sets being used are the same and check time series lengths and starting dates\n",
    "countrysets = [d_countries,c_countries,lc_countries,pc_countries,nc_countries]\n",
    "print([len(ccs) for ccs in countrysets])\n",
    "for ccs1 in countrysets:\n",
    "    print([ccs1 == ccs2 for ccs2 in countrysets])\n",
    "print([len(clusdata_all[d1]['United States']) for d1 in datasets])\n",
    "# print(len(total_deaths_x['dates']),len(total_cases_x['dates']),len(testing_x['dates']),total_deaths_x['dates'][0],total_cases_x['dates'][0],testing_x['dates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 61 countries on Sep 24 with mindeaths=200 and mindays=160\n",
    "# note that changing the minimal common active epidemic time series length mindays from 160 to 150 included only 4 new countries on Sep 24\n",
    "# decreasing minimal total death count from 200 to 100 included only 3 new countries on Sep 24 \n",
    "# doing both included 7 new countries : 68 in total \n",
    "len(testing_x['dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_owid[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_ts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_owid_ts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fft / filter play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in covid_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in covid_ts['deaths'] if 'US' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = covid_ts['new_deaths'][('US','')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(foo);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "foo = foo[:259]\n",
    "fftdat = np.fft.rfft(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(foo);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 31\n",
    "hwidth = int(width/2)\n",
    "# windows:\n",
    "# bartlett (triangle)\n",
    "# blackman, hamming, hanning (cos hump with different size tails)\n",
    "# kaiser (from bessel, with beta param = 0 rectangula, 5 hamming, 6 hanning, 8.6 blackman)\n",
    "# cf.  https://numpy.org/doc/stable/reference/routines.window.html\n",
    "\n",
    "wfoo = 1-np.kaiser(width,15)\n",
    "wfoo = 1-np.blackman(width)\n",
    "\n",
    "win = 1+np.zeros(len(fftdat))\n",
    "\n",
    "for k in [37,74,111]:\n",
    "    for i in range(k-hwidth,k+hwidth):\n",
    "        win[i] = wfoo[i-k+hwidth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtdat = [fftdat[i]*win[i] for i in range(len(win))]\n",
    "\n",
    "smoothed = np.fft.irfft(filtdat)\n",
    "rollingave = foo.copy()\n",
    "rollingave = [np.mean(foo[(i-7):i]) for i in range(7,len(foo))]\n",
    "\n",
    "plt.plot(foo);\n",
    "plt.plot(rollingave,color='black')\n",
    "plt.plot(smoothed,color='red',alpha=0.8);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison and cleaning of data : JHU and OWID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country list translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print([cc[0] for cc in countries_jhu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries_jhu_str_totals= [cc[0] for cc in countries_jhu if cc[1] == 'Total']\n",
    "countries_jhu_str = [cc[0] for cc in countries_jhu if cc[0] not in countries_jhu_str_totals ]\n",
    "countries_jhu_str = countries_jhu_str + countries_jhu_str_totals\n",
    "set(countries_jhu_str)-set(countries_owid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries_jhu_str_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jhu_to_owid_country = {\n",
    "    'Burma':'Myanmar',\n",
    "    'Cabo Verde':'Cape Verde',\n",
    "    'Congo (Brazzaville)':'Congo',\n",
    "    'Congo (Kinshasa)':'Democratic Republic of Congo',\n",
    "    'Czechia':'Czech Republic',\n",
    "    'Diamond Princess':'Diamond Princess',\n",
    "    'Eswatini':'Swaziland',\n",
    "    'Holy See':'Vatican',\n",
    "    'Korea, South':'South Korea',\n",
    "    'MS Zaandam':'MS Zaandam',\n",
    "    'North Macedonia':'Macedonia',\n",
    "    'Taiwan*':'Taiwan',\n",
    "    'Timor-Leste':'Timor',\n",
    "    'US':'United States',\n",
    "    'West Bank and Gaza':'Palestine'\n",
    "}\n",
    "for cc in countries_owid:\n",
    "    jhu_to_owid_country.update({cc:cc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "population_owid.keys()\n",
    "print(len(population_owid.keys()))\n",
    "for cc in population_owid:\n",
    "    print(cc,population_owid[cc][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries_jhu_t_str=[jhu_to_owid_country[cc[0]] for cc in countries_jhu]\n",
    "set(countries_jhu_t_str)-set(countries_owid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(countries_owid)-set(countries_jhu_t_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(countries)-set(countries_jhu_t_str)  # check that all countries in our reduced list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that Puerto Rico is a territory of the US\n",
    "and JHU has it split up into the lower admin levels and only in the separate US database csv files.\n",
    "We would need to sum these to get Puerto Rico and then only available confirmed and deaths not recovered.\n",
    "I have not attempted this so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_owid['Puerto Rico'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison dates set for data JHU and OWID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('jhu',covid_ts['deaths']['dates'][0],covid_ts['deaths']['dates'][-1])\n",
    "print('owid',covid_owid_ts['deaths']['dates'][0],covid_owid_ts['deaths']['dates'][-1])\n",
    "print('difference in length',len(covid_owid_ts['deaths']['dates'])-len(covid_ts['deaths']['dates']))\n",
    "# the next line gives false, becuas eof single day adjustment\n",
    "print('calculated common portion the same',covid_owid_ts['deaths']['dates']==covid_ts['deaths']['dates'])\n",
    "print('this is OK (false) because dates neeed to be shifted by one day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of deaths and confirmed case data JHU and OWID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_ts['deaths_daily'] = {}\n",
    "covid_ts['deaths_daily_smoothed'] = {}\n",
    "for cc in covid_ts['deaths']:\n",
    "    if cc is not 'dates':\n",
    "        tseries = covid_ts['deaths'][cc]\n",
    "        diff = np.zeros(len(tseries),dtype=float)\n",
    "        for t,s in enumerate(tseries):\n",
    "            if t == 0:\n",
    "                diff[t]= s-0.\n",
    "            else:\n",
    "                diff[t] = s - tseries[t-1]\n",
    "        covid_ts['deaths_daily'].update({cc : diff})\n",
    "        \n",
    "        week = 0.  \n",
    "        rollav7 = np.zeros(len(tseries),dtype=float)\n",
    "        for t,s in enumerate(diff):\n",
    "            week = week + s\n",
    "            if t >= 7:\n",
    "                week = week - diff[t-7]\n",
    "                rollav7[t] = week/7.\n",
    "            else:\n",
    "                rollav7[t] = week/(t+1)\n",
    "        covid_ts['deaths_daily_smoothed'].update({cc : rollav7})\n",
    "\n",
    "covid_owid_ts['deaths_daily'] = {}\n",
    "covid_owid_ts['deaths_daily_smoothed'] = {}\n",
    "for cc in covid_owid_ts['deaths']:\n",
    "    if cc is not 'dates':\n",
    "        tseries = covid_owid_ts['deaths'][cc]\n",
    "        diff = np.zeros(len(tseries),dtype=float)\n",
    "        for t,s in enumerate(tseries):\n",
    "            if t == 0:\n",
    "                diff[t]= s-0.\n",
    "            else:\n",
    "                diff[t] = s - tseries[t-1]\n",
    "        covid_owid_ts['deaths_daily'].update({cc : diff})\n",
    "        \n",
    "        week = 0.  \n",
    "        rollav7 = np.zeros(len(tseries),dtype=float)\n",
    "        for t,s in enumerate(diff):\n",
    "            week = week + s\n",
    "            if t >= 7:\n",
    "                week = week - diff[t-7]\n",
    "                rollav7[t] = week/7.\n",
    "            else:\n",
    "                rollav7[t] = week/(t+1)\n",
    "        covid_owid_ts['deaths_daily_smoothed'].update({cc : rollav7})         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country = 'Spain'\n",
    "country_jhu = (country,'')\n",
    "d1 = np.array(covid_ts['new_deaths_corrected_smoothed'][country_jhu][:]).astype(float)\n",
    "d2 = np.array(covid_owid_ts['new_deaths_corrected_smoothed'][country][:]).astype(float)\n",
    "\n",
    "#print('jhu',d1)\n",
    "#print('owid',d2)\n",
    "#print('jhu-owid',d1-d2)\n",
    "fig,axes = plt.subplots(1,1,figsize=(16,8))\n",
    "axes.plot(range(len(d1)),d1)\n",
    "axes.plot(range(len(d2)),d2)\n",
    "ax20 = axes.twinx()\n",
    "ax20.plot(range(len(d2)),d1-d2,c='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "country_jhu = (country,'')\n",
    "d1 = np.array(covid_ts['deaths_daily_smoothed'][country_jhu][:]).astype(float)\n",
    "d2 = np.array(covid_owid_ts['deaths_daily_smoothed'][country][:])\n",
    "\n",
    "#print('jhu',d1)\n",
    "#print('owid',d2)\n",
    "#print('jhu-owid',d1-d2)\n",
    "fig,axes = plt.subplots(1,1,figsize=(16,8))\n",
    "axes.plot(range(len(d1)),d1)\n",
    "axes.plot(range(len(d2)),d2)\n",
    "ax20 = axes.twinx()\n",
    "ax20.plot(range(len(d2)),d1-d2,c='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covid_ts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_corrections(country='Peru',dtype='deaths',database='jhu'):\n",
    "    global countries_jhu_str_totals\n",
    "    new_dtype = 'new_'+ dtype\n",
    "    new_dtype_corrected =  new_dtype + '_corrected'\n",
    "    new_dtype_smoothed =  new_dtype + '_smoothed'\n",
    "    new_dtype_corrected_smoothed =  new_dtype + '_corrected' + '_smoothed'\n",
    "    if database == 'jhu':\n",
    "        if country in countries_jhu_str_totals:\n",
    "            country_f = (country,'Total')\n",
    "        else:\n",
    "            country_f = (country,'')\n",
    "        covidts = covid_ts\n",
    "    else:\n",
    "        covidts = covid_owid_ts\n",
    "        country_f = country\n",
    "    d0 = np.array(covidts[new_dtype][country_f][:]).astype(float)\n",
    "    d1 = np.array(covidts[new_dtype_corrected][country_f][:]).astype(float)\n",
    "    d2 = np.array(covidts[new_dtype_smoothed][country_f][:]).astype(float)\n",
    "    d3 = np.array(covidts[new_dtype_corrected_smoothed][country_f][:]).astype(float)\n",
    "    fig,axes = plt.subplots(1,1,figsize=(24,8))\n",
    "    axes.plot(range(len(d0)),d0,label='raw')\n",
    "    axes.plot(range(len(d1)),d1,label='corrected',alpha=0.3,linewidth=5)\n",
    "    axes.plot(range(len(d2)),d2,label='smoothed',alpha=0.3,linewidth=5)\n",
    "    axes.plot(range(len(d3)),d3,label='corrected smoothed')\n",
    "    # axes.set_ylim((-10.,500.))\n",
    "    axes.set_xticks(np.linspace(0.,len(d2)-len(d2)%10,int((len(d2)-len(d2)%10)/10.+1)))\n",
    "    #ax2 = axes.twinx()\n",
    "    #ax2.plot(range(len(d2)),d2-d3,c='grey')\n",
    "    axes.legend()\n",
    "    axes.set_title(country+' '+dtype+' comparison')\n",
    "    fig,axes = plt.subplots(1,1,figsize=(24,8))\n",
    "    axes.plot(range(len(d3)),d3)\n",
    "    axes.set_title(country+' '+dtype+' corrected smoothed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corrections('Peru','deaths')\n",
    "plot_corrections('Peru','confirmed')\n",
    "plot_corrections('Spain','deaths')\n",
    "plot_corrections('Spain','confirmed')\n",
    "plot_corrections('France','deaths')\n",
    "plot_corrections('France','confirmed')\n",
    "plot_corrections('Sweden','deaths')\n",
    "plot_corrections('Sweden','confirmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corrections('France','deaths',database='owid')\n",
    "plot_corrections('France','confirmed',database='owid')\n",
    "plot_corrections('Sweden','deaths',database='owid')\n",
    "plot_corrections('Sweden','confirmed',database='owid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_corrections('Australia','deaths')\n",
    "plot_corrections('Australia','confirmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClusterFit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argsort(list(range(4,14)))[-1:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16000 2 seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base = '../../covid-19-JH/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "confirmed_jhu = get_data(base+'time_series_covid19_confirmed_global.csv')\n",
    "deaths_jhu = get_data(base+'time_series_covid19_deaths_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[x for x in dir() if 'death' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country = 'Denmark'\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(new_cases_spm[country])\n",
    "axx = ax.twinx()\n",
    "axx.plot(new_deaths_spm[country],color='red');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(new_cases_spm[country])\n",
    "axx = ax.twinx()\n",
    "axx.plot(new_deaths_spm[country],color='red');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country = 'France'\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(new_cases_spm[country])\n",
    "axx = ax.twinx()\n",
    "axx.plot(new_deaths_spm[country],color='red');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_all2(countries,dat,adj=None,testing=None,ndays=250):\n",
    "    max_cols=6\n",
    "    max_rows=int(len(countries)/max_cols) + 1\n",
    "    fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(24,4*max_rows))\n",
    "\n",
    "    for idx, country  in enumerate(countries):\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        axes[row,col].plot(dat[country])\n",
    "        axes[row,col].set_title(country)\n",
    "        if adj is not None:\n",
    "            ax = axes[row,col].twinx()\n",
    "            ax.plot(adj[country],color='darkred',alpha=0.6)\n",
    "    for idx in range(len(countries),max_rows*max_cols):\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        axes[row, col].axis(\"off\")\n",
    "    #plt.subplots_adjust(wspace=.05, hspace=.05)\n",
    "    fig.tight_layout()\n",
    "    #for ax in fig.get_axes():\n",
    "    #    ax.label_outer()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all2(countries,new_cases_spm,new_deaths_spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(confirmed_jhu[('Germany','')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clusdata_all['cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#foo = ClusterFit(clusdata_all['cases'],fft='fft') # real and imag components of FFT used together for PCA\n",
    "#foo = ClusterFit(clusdata_all['cases'],fft='powfft') # power spectrum via FFT used for PCA (not invertible)\n",
    "foo = ClusterFit(clusdata_all['cases'],fft=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.umap_cluster(5,5,n_neighbors=6)\n",
    "print('mean cluster membership probability =',np.mean(foo.clus_probs))\n",
    "foo.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.clus_labels\n",
    "print(len(foo.clus_labels))\n",
    "print(len(c_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.clus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labelled = [(foo.clus_labels[i],c_countries[i]) for i in range(len(foo.clus_labels))]\n",
    "print(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.lexsort((c_countries,foo.clus_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# foo.umap_best_cluster(Nclus=3)\n",
    "foo.umap_best_cluster(Nclus=4,n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(foo.clus_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.clus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.cluster_plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.plot_pcas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clsize = []\n",
    "clprobs = []\n",
    "for i in range(100):\n",
    "    foo.umap_cluster(i,min_size=5,diag=False)\n",
    "    clsize.append(len(set(foo.clus_labels)))\n",
    "    clprobs.append(np.mean(foo.clus_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(clsize,clprobs)\n",
    "plt.title('min_size=5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# min_size = 6\n",
    "plt.scatter(clsize,clprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusfit_all = {}\n",
    "clusfit_all['cases'] = foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## cases FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clusdata_all['cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft = ClusterFit(clusdata_all['cases'],fft='fft') # real and imag components of FFT used together for PCA\n",
    "#foo = ClusterFit(clusdata_all['cases'],fft='powfft') # power spectrum via FFT used for PCA (not invertible)\n",
    "#foo = ClusterFit(clusdata_all['cases'],fft=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.umap_cluster(5,5)\n",
    "print('mean cluster membership probability =',np.mean(foofft.clus_probs))\n",
    "foofft.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# foo.umap_best_cluster(Nclus=3)\n",
    "foofft.umap_best_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(foofft.clus_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.clus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.cluster_plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.plot_pcas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clsize = []\n",
    "clprobs = []\n",
    "for i in range(100):\n",
    "    foofft.umap_cluster(i,min_size=5,diag=False)\n",
    "    clsize.append(len(set(foofft.clus_labels)))\n",
    "    clprobs.append(np.mean(foofft.clus_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(clsize,clprobs)\n",
    "plt.title('min_size=5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# min_size = 6\n",
    "plt.scatter(clsize,clprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusfit_all = {}\n",
    "clusfit_all['cases'] = foofft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clusdata_all['deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = ClusterFit(clusdata_all['deaths']) # real and imag components of FFT used together for PCA\n",
    "#foo = ClusterFit(clusdata_all['deaths'],fft='powfft') # power spectrum via FFT used for PCA (not invertible)\n",
    "#foo = ClusterFit(clusdata_all['deaths'],fft=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.umap_cluster(5,5)\n",
    "print('mean cluster membership probability =',np.mean(foofft.clus_probs))\n",
    "foo.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# foo.umap_best_cluster(Nclus=3)\n",
    "foo.umap_best_cluster(Nclus=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(foo.clus_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.cluster_plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.plot_pcas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clsize = []\n",
    "clprobs = []\n",
    "for i in range(100):\n",
    "    foo.umap_cluster(i,min_size=5,diag=False)\n",
    "    clsize.append(len(set(foo.clus_labels)))\n",
    "    clprobs.append(np.mean(foo.clus_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(clsize,clprobs)\n",
    "plt.title('min_size=5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# min_size = 6\n",
    "plt.scatter(clsize,clprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);\n",
    "plt.show()\n",
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(clprobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusfit_all = {}\n",
    "clusfit_all['cases'] = foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deaths FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft = ClusterFit(clusdata_all['deaths'],fft='fft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.umap_cluster(3) # arg = random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.umap_best_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.plot_umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foofft.plot_pcas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusfit_all['deaths'] = foofft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plots of all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(countries,clusdata_all['deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(countries,clusdata_all['cases'],clusdata_all['cases_nonlin'],longshort_testing_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(countries,clusdata_all['cases'],clusdata_all['cases_nonlinr'],longshort_reg_testing_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(countries,clusdata_all['cases'],clusdata_all['cases_pwlfit'],longshort_reg_testing_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(lcountries,cases_raw,cases_adj_lin2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare best clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [c for c in clusdata_all]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all(countries,clusdata_all['cases'],clusdata_all['cases_nonlinr']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Norman not sure I understand what you are printing out with \"found 48 clusters of size 3\" , seems wrong\n",
    "res = {}\n",
    "for d in datasets:\n",
    "    print('doing ',d,'...')\n",
    "    mfit = ClusterFit(clusdata_all[d])\n",
    "    mfit.umap_best_cluster()\n",
    "    res[d] = mfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rr in res:\n",
    "    foo = res[rr]\n",
    "    # foo.plot_umap() ########## doesn't work???\n",
    "    plt.scatter(foo.um_dat[0],foo.um_dat[1],c=foo.clus_labels)\n",
    "    plt.title(rr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corcl(a,b):\n",
    "    if len(set(a)) > 0 or len(set(b)) > 0:\n",
    "        return len(set(a).intersection(set(b)))/float(len(set(a).union(set(b))))\n",
    "    else:\n",
    "        return 1 \n",
    "    \n",
    "def matchset(a,x):\n",
    "    rtn = [i for i in range(len(a)) if a[i] == x]\n",
    "    return rtn\n",
    "\n",
    "def match1(a,x):\n",
    "    rtn = [1 for i in range(len(a)) if a[i] == x]\n",
    "    return rtn\n",
    "    \n",
    "def imxcor(clusters,nset,n,nclus,nclusmax=6): \n",
    "    \"\"\" finds cluster index in previous datasets d best correlated to cluster n in dataset dd\n",
    "        nclus is current number of already aligned clusters\n",
    "        nclusmax is max allowed number of clusters\n",
    "    \"\"\"\n",
    "    cx = []\n",
    "    cc = clusters[nset]\n",
    "    for j in range(0,nset):\n",
    "        c = mapclusters[j]\n",
    "        klen = min(nclusmax,len(cc))\n",
    "        for k in range(nclus): # find best matching previous cluster\n",
    "            cx.append(np.sum(match1(cc,n)*match1(c,k)))\n",
    "    return argmax(cx),max(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxclusters = 6\n",
    "clusters = [res[d].clus_labels for d in res]\n",
    "ncountries = len(res['deaths'])\n",
    "mapclusters = np.zeros((len(clusters),ncountries)\n",
    "mapclusters[0,:] = clusters[0,:] # class lables are those of deaths to start, may be expanded\n",
    "corclasses = np.zeros((len(classes),len(classes)))\n",
    "c = clusters[0] # the first dataset clusters are all accepted as first clusters\n",
    "nclusters = len(set(c))                    \n",
    "for j in range(1,len(clusters)):\n",
    "    cc = clusters[j] # clus_labels for jth dataset\n",
    "                      # map clusters to best matching cluster in 0th dataset : losers try with other previous datasets : losers new\n",
    "    for k in set(cc):\n",
    "                       i,cor = imxcor(k,cc,nclusmax=nclasses)\n",
    "                       \n",
    "    corclasses[i,j] = mxcor(c,cc)\n",
    "    corclasses[j,i] = corclasses[i,j]\n",
    "for i in range(len(classes)):\n",
    "    corclasses[i,i] = 1.0\n",
    "plt.imshow(corclasses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([(i,c,np.mean(corclasses[i,:])) for i,c in enumerate([r for r in res])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    plt.plot(list(range(6)),corclasses[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `cases_nonlin` and `cases_nonlinr` currently same.\n",
    "\n",
    "**Cluster correlations not outstandingly high!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = res['cases_nonlin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_adj(country, data, adj = None, testing=None,  ndays=250, axis = None):\n",
    "    ndays = 250\n",
    "    if testing:\n",
    "        # Ntests = regtests(testing,country)  # this does not work here, since data is already synchronized, use regularized data in testing \n",
    "        Ntests = testing[country]\n",
    "    if axis is None:   \n",
    "        fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "    else:\n",
    "        ax1 = axis\n",
    "    ax1.plot(data[country][:ndays]) \n",
    "    if adj is not None:  # already adjusted\n",
    "        ax1.plot(adj[country][:ndays])\n",
    "    ax1.set_title(country)\n",
    "    ax1.set_ylabel('Cases/million')\n",
    "    ax1.set_xlabel('day')\n",
    "    if testing:\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(Ntests[:ndays],color='red',alpha=0.4)\n",
    "        ax2.set_ylabel('Testing/1000')\n",
    "\n",
    "        \n",
    "def plot_all(countries,dat,adj=None,testing=None,ndays=250):\n",
    "    max_cols=6\n",
    "    max_rows=int(len(countries)/max_cols) + 1\n",
    "    fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(24,4*max_rows))\n",
    "\n",
    "    for idx, country  in enumerate(countries):\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        plot_adj(country,dat,adj,testing,ndays,axis=axes[row,col])\n",
    "    for idx in range(len(countries),max_rows*max_cols):\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        axes[row, col].axis(\"off\")\n",
    "    #plt.subplots_adjust(wspace=.05, hspace=.05)\n",
    "    fig.tight_layout()\n",
    "    #for ax in fig.get_axes():\n",
    "    #    ax.label_outer()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(foo.smoothed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(foo.dat[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat1 = {countries[i]:foo.dat[i,:] for i in range(len(countries))}\n",
    "dat2 = {countries[i]:foo.smoothed[i,:] for i in range(len(countries))}\n",
    "plot_all(countries,dat1,dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc = 'cases_nonlin'\n",
    "clus0 = [i for i,x in enumerate(res[cc].clus_labels) if x==0]\n",
    "\n",
    "count0 = [countries[i] for i in clus0]\n",
    "plot_all(count0,dat1,dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc = 'cases_nonlin'\n",
    "clus = [i for i,x in enumerate(res[cc].clus_labels) if x==1]\n",
    "\n",
    "count = [countries[i] for i in clus]\n",
    "plot_all(count,dat1,dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc = 'cases_nonlin'\n",
    "clus = [i for i,x in enumerate(res[cc].clus_labels) if x==2]\n",
    "\n",
    "count = [countries[i] for i in clus]\n",
    "plot_all(count,dat1,dat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hdbscan on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First view of HDBSCAN, a hierarchical clustering algorithm, applied here to raw data.\n",
    "\n",
    "HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander. \n",
    "\n",
    "It extends DBSCAN by converting it into a hierarchical clustering algorithm, \n",
    "and then using a technique to extract a flat clustering based in the stability of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath = ClusterFit(clusdata_all['deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.hdbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.umap(n_neighbors=4)  # execute the umap projection on the death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(foodeath.clus_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.clus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.plot_umap() # our plot routine in the ClusterFit class uses the clus_labels to colour points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two clusters: the red points (-1 values) are unclustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hdbscan on raw PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we perform hierarchical clustering on the PCA projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath = ClusterFit(clusdata_all['deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.hdbscan_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.umap(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(foodeath.clus_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.clus_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.plot_umap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two clusters: the red points (-1 values) are unclustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Example](https://fda.readthedocs.io/en/latest/auto_examples/plot_fpca.html#sphx-glr-auto-examples-plot-fpca-py) of FPCA...\n",
    "\n",
    "Also: see same example in the fpca_example notebook.\n",
    "\n",
    "Also:  see [documentation](https://fda.readthedocs.io/en/latest/modules/preprocessing/dim_reduction/autosummary/skfda.preprocessing.dim_reduction.projection.FPCA.html?highlight=fpca) for FPCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using raw data (as discrete samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary scratch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = foodeath.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foodeath.fitted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import skfda\n",
    "from skfda.datasets import fetch_growth\n",
    "from skfda.exploratory.visualization import plot_fpca_perturbation_graphs\n",
    "from skfda.preprocessing.dim_reduction.projection import FPCA\n",
    "from skfda.representation.basis import BSpline, Fourier, Monomial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure data a samples of a function on a grid, specify the grid as list of days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dat_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_disc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = [i for i,x in enumerate(countries) if x==\"Spain\"][0]\n",
    "dat_disc[idx].plot()\n",
    "plt.title(countries[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpca_disc = FPCA(n_components=10)\n",
    "fpca_disc.fit(dat_disc)\n",
    "fpca_disc.components_.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_cols = 5\n",
    "max_rows = len(fpca_disc.components_) // max_cols\n",
    "#fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(20,max_rows*3.5))\n",
    "for i in range(len(fpca_disc.components_)):\n",
    "    row = i // max_cols\n",
    "    col = i % max_cols\n",
    "    #axes[row, col].axis(\"off\")\n",
    "    fpca_disc.components_[i].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = fpca_disc.transform(dat_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[x for x in dir(fpca_disc) if 'transform' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for minc in range(2,8):\n",
    "    for ncomp in range(5,11):\n",
    "        fpca_disc = FPCA(n_components=ncomp)\n",
    "        fpca_disc.fit(dat_disc)\n",
    "        foo = fpca_disc.transform(dat_disc)\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=minc)\n",
    "        labels = clusterer.fit_predict(foo)\n",
    "        try:\n",
    "            validity = hdbscan.validity.validity_index(foo, labels)\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "                  sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',np.round(validity,3))\n",
    "        except:\n",
    "            validity=None\n",
    "\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "              len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "              sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',validity)\n",
    "        #print(labels)\n",
    "\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_samples = 3\n",
    "for minc in range(2,8):\n",
    "    for ncomp in range(3,11):\n",
    "        fpca_disc = FPCA(n_components=ncomp)\n",
    "        fpca_disc.fit(dat_disc)\n",
    "        foo = fpca_disc.transform(dat_disc)\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples)\n",
    "        labels = clusterer.fit_predict(foo)\n",
    "        try:\n",
    "            validity = hdbscan.validity.validity_index(foo, labels)\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "                  sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',np.round(validity,3))\n",
    "        except:\n",
    "            validity=None\n",
    "\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "              len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "              sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',validity)\n",
    "        #print(labels)\n",
    "\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### integrated into ClusterFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = ClusterFit(clusdata_all['deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.hdbscan_fpca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.umap()\n",
    "foo.plot_umap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foocases = ClusterFit(clusdata_all['cases'])\n",
    "dat = foocases.dat\n",
    "dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "foo = fpca_disc.transform(dat_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_samples = 4\n",
    "for minc in range(2,8):\n",
    "    for ncomp in range(3,11):\n",
    "        fpca_disc = FPCA(n_components=ncomp)\n",
    "        fpca_disc.fit(dat_disc)\n",
    "        foo = fpca_disc.transform(dat_disc)\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples)\n",
    "        labels = clusterer.fit_predict(foo)\n",
    "        try:\n",
    "            validity = hdbscan.validity.validity_index(foo, labels)\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "                  sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',np.round(validity,3))\n",
    "        except:\n",
    "            validity=None\n",
    "\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "              len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "              sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',validity)\n",
    "        #print(labels)\n",
    "\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases adj_nonlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foocases_nonlin = ClusterFit(clusdata_all['cases_nonlin'])\n",
    "dat = foocases_nonlin.dat\n",
    "dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "foo = fpca_disc.transform(dat_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_samples = 3\n",
    "for minc in range(2,8):\n",
    "    for ncomp in range(3,16):\n",
    "        fpca_disc = FPCA(n_components=ncomp)\n",
    "        fpca_disc.fit(dat_disc)\n",
    "        foo = fpca_disc.transform(dat_disc)\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples,gen_min_span_tree=True)\n",
    "        labels = clusterer.fit_predict(foo)\n",
    "        try:\n",
    "            validity = hdbscan.validity.validity_index(foo, labels)\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "                  sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',np.round(validity,5))\n",
    "        except:\n",
    "            validity=None\n",
    "\n",
    "            print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "              len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "              sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',validity)\n",
    "        #print(labels)\n",
    "\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan of optimal clusterings for 6 datasets and 4 clustering quality criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validity calculated by hdbscan does seem to bias lower dimensional embeddings. We study this in a separate notebook \"Calibration_of_Validity\". Here we consider 4 scorings: the validity, the dimensionality scaled validity, and two combined scorings which also penalizes the number of unclustered points as well as how far the number of clusters departs from 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** move Ncomponents to outside loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rescale(v,d):\n",
    "    \"\"\" functional form of correction factor using simple inversion formula\n",
    "        for with v2'=1/(1-v2) the dimensionality correction v = v2 * v2'/(v2'+d/2-1)\n",
    "        projecting equivalent validity at dim = 2\"\"\"\n",
    "    if d > 12.:\n",
    "        d = 12.\n",
    "    logd = np.log(d/2.)\n",
    "    return v*(1.+logd)/(1.+v*logd)\n",
    "\n",
    "maxvalid = [None,None,None,None,None,None]\n",
    "maxvalidval= 0.0\n",
    "maxvalidsc = [None,None,None,None,None,None]\n",
    "maxvalidscval= 0.0\n",
    "minscore1 = [None,None,None,None,None,None]\n",
    "minscore1val = 999.\n",
    "minscore2 = [None,None,None,None,None,None]\n",
    "minscore2val = 999.\n",
    "report = [' ']*4*6\n",
    "probdata=np.zeros((4*6,len(dat)),dtype=float)\n",
    "clusdata = np.zeros((4*6,len(countries)),dtype=np.integer)\n",
    "\n",
    "for ic,case in enumerate(cases):\n",
    "    foocase = ClusterFit(clusdata_all[case])\n",
    "    dat = foocase.dat\n",
    "    dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "\n",
    "    print('--------------------------',case,'-------------------------------')\n",
    "    maxvalidval= 0.0\n",
    "    maxvalidscval= 0.0\n",
    "    minscore1val = 999.\n",
    "    minscore2val = 999.\n",
    "\n",
    "    for ncomp in range(2,16):\n",
    "        min_samples = 2\n",
    "        for minc in range(3,10):\n",
    "            fpca_disc = FPCA(n_components=ncomp)\n",
    "            fpca_disc.fit(dat_disc)\n",
    "            foo = fpca_disc.transform(dat_disc)\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples)\n",
    "            labels = clusterer.fit_predict(foo)\n",
    "            nclus = len(set([x for x in labels if x>-1]))\n",
    "            nclustered = sum([1 for x in labels if x>-1])\n",
    "            nunclustered = sum([1 for x in labels if x==-1])\n",
    "\n",
    "            try:\n",
    "                validity = hdbscan.validity.validity_index(foo, labels)\n",
    "                validity = max(validity,0.001)\n",
    "                validitysc = rescale(validity,ncomp) \n",
    "                score1 = 1.0/validitysc + nunclustered/5 + np.abs(nclus-4)/2\n",
    "                score2 = nunclustered*(4.+np.abs(nclus-4))/(validitysc*20)\n",
    "\n",
    "                if validity > maxvalidval:\n",
    "                    maxvalidval = validity\n",
    "                    maxvalid[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4,:] = clusterer.probabilities_[:]\n",
    "                    clusdata[ic*4,:] = labels[:]\n",
    "                    report[ic*4] = 'max normal validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "\n",
    "                if validitysc > maxvalidscval:\n",
    "                    maxvalidscval = validitysc\n",
    "                    maxvalidsc[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+1,:] = clusterer.probabilities_[:]\n",
    "                    clusdata[ic*4+1,:] = labels[:]\n",
    "                    report[ic*4+1] = 'max scaled validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                if score1 <  minscore1val:\n",
    "                    minscore1val = score1\n",
    "                    minscore1[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]   \n",
    "                    probdata[ic*4+2,:] = clusterer.probabilities_[:]\n",
    "                    clusdata[ic*4+2,:] = labels[:]\n",
    "                    report[ic*4+2] = 'min combined score1: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                if score2 <  minscore2val:\n",
    "                    minscore2val = score2\n",
    "                    minscore2[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+3,:] = clusterer.probabilities_[:]\n",
    "                    clusdata[ic*4+3,:] = labels[:]\n",
    "                    report[ic*4+3] = 'min combined score2: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    \n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                      nclus,'clusters;  ',\n",
    "                      nclustered,'clustered;  ',\n",
    "                      nunclustered,'unclustered; ','validity =',np.round(validity,5),'validitysc =',np.round(validitysc,5),\n",
    "                      'score1:',np.round(score1,3),'score2:',np.round(score2,3))\n",
    "            except:\n",
    "                validity=None\n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  nclus,'clusters;  ',\n",
    "                  nclustered,'clustered;  ',nunclustered,'unclustered; ','validity =',validity)\n",
    "\n",
    "        print('--------------------------')\n",
    "    print('---------------------------------------------------------')\n",
    "    print('minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2')\n",
    "    print('maxvalid ',maxvalid[ic])\n",
    "    print('maxvalidsc ',maxvalidsc[ic])\n",
    "    print('minscore1',minscore1[ic])\n",
    "    print('minscore2',minscore2[ic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stash results in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def rescale(v,d):\n",
    "    \"\"\" functional form of correction factor using simple inversion formula\n",
    "        for with v2'=1/(1-v2) the dimensionality correction v = v2 * v2'/(v2'+d/2-1)\n",
    "        projecting equivalent validity at dim = 2\"\"\"\n",
    "    if d > 12.:\n",
    "        d = 12.\n",
    "    logd = np.log(d/2.)\n",
    "    return v*(1.+logd)/(1.+v*logd)\n",
    "\n",
    "maxvalid = [None,None,None,None,None,None]\n",
    "maxvalidval= 0.0\n",
    "maxvalidsc = [None,None,None,None,None,None]\n",
    "maxvalidscval= 0.0\n",
    "minscore1 = [None,None,None,None,None,None]\n",
    "minscore1val = 999.\n",
    "minscore2 = [None,None,None,None,None,None]\n",
    "minscore2val = 999.\n",
    "report = [' ']*4*6\n",
    "reportdata = [None]*4*6\n",
    "probdata=np.zeros((4*6,len(dat)),dtype=float)\n",
    "outlierdata=np.zeros((4*6,len(dat)),dtype=float)\n",
    "clusdata = np.zeros((4*6,len(countries)),dtype=np.integer)\n",
    "info =  pd.DataFrame(columns=['type','minc','mins','ncomp','clustered','unclustered','validity','validitysc','score1','score2'])\n",
    "infomax =  pd.DataFrame(columns=['type','minc','mins','ncomp','clustered','unclustered','validity','validitysc','score1','score2'])\n",
    "cnt=0\n",
    "for ic,case in enumerate(cases):\n",
    "    foocase = ClusterFit(clusdata_all[case])\n",
    "    dat = foocase.dat\n",
    "    dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "\n",
    "    print('--------------------------',case,'-------------------------------')\n",
    "    maxvalidval= 0.0\n",
    "    maxvalidscval= 0.0\n",
    "    minscore1val = 999.\n",
    "    minscore2val = 999.\n",
    "    for ncomp in range(2,16):  # code will only work if reference value 2 included in range\n",
    "        min_samples = 2\n",
    "        for minc in range(3,10):\n",
    "            fpca_disc = FPCA(n_components=ncomp)\n",
    "            fpca_disc.fit(dat_disc)\n",
    "            foo = fpca_disc.transform(dat_disc)\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples)\n",
    "            labels = clusterer.fit_predict(foo)\n",
    "            nclus = len(set([x for x in labels if x>-1]))\n",
    "            nclustered = sum([1 for x in labels if x>-1])\n",
    "            nunclustered = sum([1 for x in labels if x==-1])\n",
    "            try:\n",
    "                validity = hdbscan.validity.validity_index(foo, labels)\n",
    "                validity = max(validity,0.001)\n",
    "                validitysc = rescale(validity,ncomp) \n",
    "                score1 = 1.0/validitysc + nunclustered/5 + np.abs(nclus-4)/2\n",
    "                score2 = nunclustered*(4.+np.abs(nclus-4))/(validitysc*20)\n",
    "                if validity > maxvalidval:\n",
    "                    maxvalidval = validity\n",
    "                    maxvalid[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4,:] = labels[:]\n",
    "                    report[ic*4] = 'max normal validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if validitysc > maxvalidscval:\n",
    "                    maxvalidscval = validitysc\n",
    "                    maxvalidsc[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+1,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+1,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+1,:] = labels[:]\n",
    "                    report[ic*4+1] = 'max scaled validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+1] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if score1 <  minscore1val:\n",
    "                    minscore1val = score1\n",
    "                    minscore1[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]   \n",
    "                    probdata[ic*4+2,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+2,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+2,:] = labels[:]\n",
    "                    report[ic*4+2] = 'min combined score1: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+2] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if score2 <  minscore2val:\n",
    "                    minscore2val = score2\n",
    "                    minscore2[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+3,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+3,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+3,:] = labels[:]\n",
    "                    report[ic*4+3] = 'min combined score2: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+3] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                    \n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                      nclus,'clusters;  ',\n",
    "                      nclustered,'clustered;  ',\n",
    "                      nunclustered,'unclustered; ','validity =',np.round(validity,5),'validitysc =',np.round(validitysc,5),\n",
    "                      'score1:',np.round(score1,3),'score2:',np.round(score2,3))\n",
    "            except:\n",
    "                validity=None\n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  nclus,'clusters;  ',\n",
    "                  nclustered,'clustered;  ',nunclustered,'unclustered; ','validity =',validity)\n",
    "            info.loc[cnt] = [case,minc,min_samples,ncomp,nclustered,nunclustered,validity,validitysc,score1,score2]\n",
    "            cnt = cnt+1\n",
    "\n",
    "        print('--------------------------')\n",
    "    print('---------------------------------------------------------')\n",
    "    print('minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2')\n",
    "    print('maxvalid ',maxvalid[ic])\n",
    "    print('maxvalidsc ',maxvalidsc[ic])\n",
    "    print('minscore1',minscore1[ic])\n",
    "    print('minscore2',minscore2[ic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(info['ncomp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(info['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = info[info['ncomp']==11]['validitysc']\n",
    "plt.hist(foo,bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datlen = len(set(info['ncomp']))\n",
    "\n",
    "datlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'ncomp = {}'.format(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_cols = 6\n",
    "vals = set(info['ncomp'])\n",
    "max_rows = len(vals) // max_cols +1\n",
    "fig, axes = plt.subplots(nrows=max_rows, ncols=max_cols, figsize=(24,4*max_rows))\n",
    "\n",
    "for idx, val  in enumerate(vals):\n",
    "    row = idx // max_cols\n",
    "    col = idx % max_cols\n",
    "    axes[row,col].hist(info[info['ncomp']==val]['validitysc'])\n",
    "    axes[row,col].set_title('validitysc, with ncomp = {}'.format(val))\n",
    "    axes[row,col].set_xlim((.1,.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info['validitysc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of cluster membership probabilities_ and outlier_scores_\n",
    "Note that:\n",
    "* probabilites_ p are 0 if rated as belonging to unclustered class (-1)\n",
    "* but the outlier_scores_ are non zero for all points\n",
    "* outlier_scores_ are 1-p if the points belong to a particular cluster\n",
    "* and are larger if more likely to be an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(outlierdata))\n",
    "outlierdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(12,12))\n",
    "ax.scatter(range(len(outlierdata[0])),probdata[0],alpha=0.3)\n",
    "ax.scatter(range(len(outlierdata[0])),1-outlierdata[0],alpha=0.3)\n",
    "ax.scatter(range(len(outlierdata[0])),-clusdata[0]-0.5,alpha=0.3)\n",
    "ax.grid(b=True,which='both',axis='both')\n",
    "ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(6,4,figsize=(24,36))\n",
    "for n in range(24):\n",
    "    i = n %4\n",
    "    j = int (n/4)\n",
    "    ax = axes[j,i]\n",
    "    ax.scatter(range(len(outlierdata[0])),probdata[n],alpha=0.3)\n",
    "    ax.scatter(range(len(outlierdata[0])),1-outlierdata[n],alpha=0.3)\n",
    "    #ax.set_ylim(0.03,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probdata2 = np.where(probdata==0.,outlierdata,probdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering consensus between 6 data sets and 4 optimal clusterings per set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alignment between clusterings is complex in general. See section 7 for a more sophisticated, but NYI approach for our data. Here we analyse and compare clusterings using colour index matching. Note that we now use outlier_scores_ for unclustered countries (not 0 as before) -> gray levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refclustering = 21    # ficudial column; change here.\n",
    "clus_argsort = np.lexsort((countries,clusdata[refclustering]))  # must run a scan above to define and fill clusdata.\n",
    "scountries = [countries[clus_argsort[i]] for i in range(len(countries))]\n",
    "probdata_s = probdata2.copy()\n",
    "clusdata_s = clusdata.copy()\n",
    "for i in range(len(probdata2)):\n",
    "    foo = probdata2[i]\n",
    "    for j in range(len(scountries)):\n",
    "        probdata_s[i,j] = probdata2[i,clus_argsort[j]]\n",
    "        clusdata_s[i,j] = clusdata[i,clus_argsort[j]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(clusdata)\n",
    "clusdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdata = np.random.random((10,10))              # prob of correct assignment to chosen cluster\n",
    "rawdata = np.transpose(probdata_s)\n",
    "# cindex = np.random.random_integers(0,3,(10,10))  # cluster index \n",
    "cindex = np.transpose(clusdata_s)\n",
    "ncols = len(set(clusdata.flatten()))\n",
    "if ncols>11:\n",
    "    print('currently only 11 colours allowed', ncols )\n",
    "colors = np.array([[1,1,1],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1],[0.5,1,0],[0,1,0.5],[0.5,0,1],[0.5,1,0.5],[0.3,0.7,0.5]]) # black,red,green,blue,yellow,cyan,magenta\n",
    "colors = np.concatenate((colors,colors))\n",
    "cluscols = np.transpose(colors[cindex[:,:]+1],(2,0,1)) # transpose to allow elementwise multiplication with rawdata with separate r,g,b\n",
    "coldata = np.transpose((cluscols+3*cluscols*rawdata)/4.,(1,2,0))   # transpose back to have colours as elements of 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,20))\n",
    "img = ax.imshow(coldata)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(scountries)\n",
    "ax.set_xticks(range(len(clusdata_s)))\n",
    "plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "ax.set_xticklabels(report,rotation='vertical')\n",
    "# fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster comparison suffers from the independent ordering of clusters, which makes the colourings different in each column. In general, given the differnet number of clusters this is a nontrivial problem in graph matching. We adopt a two phase approach in what follows: \n",
    "* first choose a reference column (here column `refclustering=1` (defined in a cell above), not zero) with a good differentiated clustering.\n",
    "* relabel the clusters in each other column with the colours of the best matching cluster in the reference column (`coldata_adj`)\n",
    "* then relabel the colours again in case of split clusters, with the hybrid colour of the source cluster colour in reference column and the destination colour (`coldata_adj2`)\n",
    "\n",
    "`coldata`, `coldata_adj` and `coldata_adj2` are 3-d matrices: rows labeled by countries, columns labeled by report string (from max scoring), and 3 values for RGB in z-dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_int(a,b):\n",
    "    if len(set(a)) > 0 or len(set(b)) > 0:\n",
    "        return len(set(a).intersection(set(b)))\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def score_int_union(a,b):\n",
    "    if len(set(a)) > 0 or len(set(b)) > 0:\n",
    "        return len(set(a)&set(b))/len(set(a)|set(b))  # length intersection divided by length union\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def matchset(a,x):\n",
    "    rtn = [i for i in range(len(a)) if a[i] == x]\n",
    "    return rtn\n",
    " \n",
    "from matplotlib import colors as mpcolors\n",
    "\n",
    "def closest_hue(hue,huelist):\n",
    "    mindist = 2.\n",
    "    imin = -1\n",
    "    for i,h in enumerate(huelist):\n",
    "        if h > hue:\n",
    "            dist = min(h-hue,hue+1-h)\n",
    "        else:\n",
    "            dist = min(hue-h,h+1-hue)\n",
    "        if dist < mindist:\n",
    "            mindist = dist\n",
    "            imin = i\n",
    "    return imin\n",
    "\n",
    "def color_mean_rgb_to_hsv(rgb_colours,weights=None): \n",
    "    \"\"\" the hue is a circular quantity, so mean needs care\n",
    "        see https://en.wikipedia.org/wiki/Mean_of_circular_quantities\n",
    "    \"\"\"\n",
    "    pi = np.pi\n",
    "    eps = 0.0001\n",
    "    hsum = 0.\n",
    "    ssum = 0.\n",
    "    vsum = 0.\n",
    "    asum = 0.\n",
    "    bsum = 0.\n",
    "    wsum = 0.\n",
    "    hwsum = 0.\n",
    "    \n",
    "    if weights == None:\n",
    "        weights = [1 if mpcolors.rgb_to_hsv(c)[1] > 0 else 0 for c in rgb_colours] # designed to exclude -1 unclustered colours\n",
    "    elif weights == 'all':\n",
    "        weights = [1 for c in rgb_colours]\n",
    "        \n",
    "    for i,c in enumerate(rgb_colours):\n",
    "        hsvcol = mpcolors.rgb_to_hsv(c)\n",
    "        h = hsvcol[0]\n",
    "        s = hsvcol[1]\n",
    "        v = hsvcol[2]\n",
    "        if s > eps and v > eps:\n",
    "            asum = asum + np.sin(h*2*pi)*weights[i]\n",
    "            bsum = bsum + np.cos(h*2*pi)*weights[i]\n",
    "            hwsum = hwsum + weights[i]\n",
    "        ssum = ssum + hsvcol[1]*weights[i]\n",
    "        vsum = vsum + hsvcol[2]*weights[i]\n",
    "        wsum = wsum + weights[i]\n",
    "        \n",
    "    if hwsum > 0:\n",
    "        asum = asum/hwsum\n",
    "        bsum = bsum/hwsum\n",
    "        h = np.arctan2(asum,bsum)/(2*pi)\n",
    "        if h < 0.:\n",
    "            h = 1.+h\n",
    "    else:\n",
    "        h = 0.\n",
    "\n",
    "    s = ssum/wsum\n",
    "    v = vsum/wsum\n",
    "    # print(rgb_colours,'mean',mpcolors.hsv_to_rgb([h,s,v]))\n",
    "    if h < 0.:\n",
    "        print('error in color_mean, hue out of range',h)\n",
    "        h = 0.\n",
    "    if h > 1.:\n",
    "        print('error in color_mean, hue out of range',h)\n",
    "        h = 1.\n",
    "    return [h,s,v]\n",
    "        \n",
    "def size_order(clusterings):\n",
    "    \"\"\" relabel clusters in each clustering in order of increasing size\"\"\"\n",
    "    clusterings_o = np.zeros(clusterings.shape,dtype = int) \n",
    "    for i,clustering in enumerate(clusterings):\n",
    "        labels = list(set(clustering)-set([-1]))\n",
    "        sizes = np.zeros(len(labels),dtype = int)\n",
    "        for j,lab in enumerate(labels):\n",
    "            sizes[j] = len(matchset(clustering,lab))\n",
    "        order = np.flip(np.argsort(sizes))\n",
    "        clusterings_o[i,:] = [order[c] if c != -1 else c for c in clustering]\n",
    "    return clusterings_o\n",
    "                      \n",
    "def clust(clustering_a,clustering_b,colors_a,colors_b,relabel=True,merge=True): \n",
    "    \"\"\" relables clustering b to match clustering a\n",
    "        if more than one cluster in a optimally matches a particular cluster in b, then color of b is merger of colors in a\n",
    "        if more than one cluster in b optimally matches a particular cluster in a, then colors in a merged and split for b\n",
    "    \"\"\"\n",
    "    labels_a = list(set(clustering_a))\n",
    "    labels_b = list(set(clustering_b))\n",
    "    newcolors_b = np.zeros((len(colors_b),3),dtype=float)\n",
    "    newcolors_b[:,:] = colors_b[:,:]\n",
    "            \n",
    "    a_to_b = {}\n",
    "    b_to_a = {}\n",
    "    a_cols = {}\n",
    "    b_cols = {}\n",
    "    \n",
    "    for a in labels_a:\n",
    "        maxscore = 0\n",
    "        maxlab = -2\n",
    "        for b in labels_b:\n",
    "            score = score_int_union(matchset(clustering_a,a),matchset(clustering_b,b))\n",
    "            if score > maxscore:\n",
    "                maxscore = score\n",
    "                maxlab = b\n",
    "        a_to_b.update({a:(maxlab,maxscore)})\n",
    "    maxvals_a_to_b = [a_to_b[a][1] for a in labels_a]\n",
    "    reorder_a = np.flip(np.argsort(maxvals_a_to_b))\n",
    "    labels_a_sort = [labels_a[r] for r in list(reorder_a)]\n",
    "\n",
    "    for b in labels_b:\n",
    "        maxscore = 0\n",
    "        maxlab = -2\n",
    "        for a in labels_a:\n",
    "            score = score_int_union(matchset(clustering_a,a),matchset(clustering_b,b))\n",
    "            if score > maxscore:\n",
    "                maxscore = score\n",
    "                maxlab = a\n",
    "        b_to_a.update({b:(maxlab,maxscore)})\n",
    "    maxvals_b_to_a = [b_to_a[b][1] for b in labels_b]\n",
    "    reorder_b = np.flip(np.argsort(maxvals_b_to_a))\n",
    "    labels_b_sort = [labels_b[r] for r in list(reorder_b)]    \n",
    "\n",
    "    if relabel:    \n",
    "        for b in labels_b_sort:   # first adjust colors_b to match mapped clusters from a (transfer and merge)\n",
    "            amap = [a for a in labels_a_sort if a_to_b[a][0] == b]\n",
    "            for a in amap:\n",
    "                alist = matchset(clustering_a,a)\n",
    "                a_cols[a] = colors_a[alist[0]]\n",
    "            blist = matchset(clustering_b,b)\n",
    "            amap_t = list(set(amap)-set([-1]))\n",
    "            if len(amap_t) > 0: # some non-unclustered (ie not -1) clusters in a map to b\n",
    "                # h = sum([mpcolors.rgb_to_hsv(a_cols[a])[0] for a in amap])/len(amap) # average hue from amap\n",
    "                h = color_mean_rgb_to_hsv([a_cols[a] for a in amap_t],[a_to_b[a][1] for a in amap_t])[0]\n",
    "                for j in blist:\n",
    "                    s = mpcolors.rgb_to_hsv(colors_b[j])[1] # take s saturation from b\n",
    "                    v = mpcolors.rgb_to_hsv(colors_b[j])[2] # take v from b\n",
    "                    newcolors_b[j,:] = mpcolors.hsv_to_rgb([h,s,v]) # back to rgb  \n",
    "            b_cols[b] = newcolors_b[blist[0]] # first matching elt colour (to extract hue)\n",
    "            \n",
    "    if merge:\n",
    "        for a in labels_a_sort:   # now readjust colors in b that both map to same a (split)\n",
    "            bmap = [b for b in labels_b_sort if b_to_a[b][0] == a]\n",
    "            if len(bmap)>1:                          \n",
    "                for i,b in enumerate(bmap):\n",
    "                    blist = matchset(clustering_b,b)\n",
    "                    # h = (mpcolors.rgb_to_hsv(b_cols[b])[0] + mpcolors.rgb_to_hsv(a_cols[a])[0])/2\n",
    "                    h = color_mean_rgb_to_hsv([b_cols[b],a_cols[a]])[0]\n",
    "                    for j in blist:                     \n",
    "                        s = mpcolors.rgb_to_hsv(b_cols[b])[1] # take s saturation from b\n",
    "                        v = mpcolors.rgb_to_hsv(b_cols[b])[2] # take v from b\n",
    "                        newcolors_b[j,:]= mpcolors.hsv_to_rgb([h,s,v])\n",
    "\n",
    "    return newcolors_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdata = np.random.random((10,10))              # prob of correct assignment to chosen cluster\n",
    "rawdata = np.transpose(probdata_s)\n",
    "# cindex = np.random.random_integers(0,3,(10,10))  # cluster index \n",
    "clusdata_s = size_order(clusdata_s)\n",
    "cindex = np.transpose(clusdata_s)\n",
    "ncols = len(set(clusdata.flatten()))\n",
    "if ncols>11:\n",
    "    print('currently only 11 colours allowed', ncols )\n",
    "colors = np.array([[1,1,1],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1],[0.5,1,0],[0,1,0.5],[0.5,0,1],[0.5,1,0.5],[0.3,0.7,0.5]]) # black,red,green,blue,yellow,cyan,magenta\n",
    "colors = np.concatenate((colors,colors))\n",
    "cluscols = np.transpose(colors[cindex[:,:]+1],(2,0,1)) # transpose to allow elementwise multiplication with rawdata with separate r,g,b\n",
    "coldata = np.transpose((cluscols+3*cluscols*rawdata)/4.,(1,2,0))   # transpose back to have colours as elements of 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpcolors.hsv_to_rgb(color_mean_rgb_to_hsv([[0,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncover problem with arctan2 function, need if correction\n",
    "for h in np.linspace(0.,1.,60):\n",
    "    a = np.sin(h*2*np.pi)\n",
    "    b = np.cos(h*2*np.pi)\n",
    "    h1 = np.arctan2(a,b)/(2*np.pi)\n",
    "    if h1 < 0.:\n",
    "        h2 = 1.+h1\n",
    "    else:\n",
    "        h2 = h1\n",
    "    # print(h,h2,h1,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coldata_c = coldata.copy()\n",
    "coldata_t = np.transpose(coldata_c,(1,0,2))\n",
    "\n",
    "print(np.shape(clusdata_s))\n",
    "print(np.shape(coldata))\n",
    "print(np.shape(coldata_t))\n",
    "\n",
    "clusa = clusdata_s[refclustering]\n",
    "ca = coldata_t[refclustering]\n",
    "for i in range(0,len(clusdata_s)):\n",
    "    if i != refclustering:\n",
    "        clusb = clusdata_s[i]\n",
    "        cb = coldata_t[i]\n",
    "        newcolors_b = clust(clusa,clusb,ca,cb,True,False)\n",
    "        coldata_t[i,:] = newcolors_b[:]\n",
    "coldata_adj = np.transpose(coldata_t,(1,0,2))\n",
    "\n",
    "coldata_c2 = coldata.copy()\n",
    "coldata_t2 = np.transpose(coldata_c2,(1,0,2))\n",
    "\n",
    "clusa = clusdata_s[refclustering]\n",
    "ca = coldata_t2[refclustering]\n",
    "for i in range(0,len(clusdata_s)):\n",
    "    if i != refclustering:\n",
    "        clusb = clusdata_s[i]\n",
    "        cb = coldata_t2[i]\n",
    "        newcolors_b = clust(clusa,clusb,ca,cb,True,True)\n",
    "        coldata_t2[i,:] = newcolors_b[:]\n",
    "coldata_adj2 = np.transpose(coldata_t2,(1,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the three stages of cluster alignment\n",
    "fig,axes = plt.subplots(1,3,figsize=(15,20))\n",
    "\n",
    "ax = axes[0]\n",
    "img = ax.imshow(coldata)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(scountries)\n",
    "ax.set_xticks(range(len(clusdata_s)))\n",
    "plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "ax.set_xticklabels(report,rotation='vertical')\n",
    "\n",
    "ax = axes[1]\n",
    "img = ax.imshow(coldata_adj)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(scountries)\n",
    "ax.set_xticks(range(len(clusdata_s)))\n",
    "plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "ax.set_xticklabels(report,rotation='vertical')\n",
    "\n",
    "ax = axes[2]\n",
    "img = ax.imshow(coldata_adj2)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(scountries)\n",
    "ax.set_xticks(range(len(clusdata_s)))\n",
    "plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "ax.set_xticklabels(report,rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the conclusions are: \n",
    "* the relabelling phases 1 and 2 are now working\n",
    "* chosing the right reference column helps : here 7 not 0\n",
    "* there is pretty good cohesion in the 3 cluster interpretation with the blue cluster confused by a lot of countries on the blue-green divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(range(10)) - set(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(clusdata_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the final cluster alignment\n",
    "def plot_clusalign(countries,data,report,cols=None):\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,24))\n",
    "    if cols is not None:\n",
    "        todel = list(set(range(data.shape[1])) - set(cols))\n",
    "        data1 = np.delete(data,todel,1)\n",
    "    else:\n",
    "        data1 = data\n",
    "    img = ax.imshow(data1)\n",
    "    ax.set_yticks(range(len(countries)))\n",
    "    ax.set_yticklabels(countries)\n",
    "    if cols is None:\n",
    "        rep = report\n",
    "    else:\n",
    "        rep = [report[i] for i in cols]\n",
    "    ax.set_xticks(range(len(rep)))\n",
    "    plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "    ax.set_xticklabels(rep,rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusalign(scountries,coldata_adj2,report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#choose columns\n",
    "cols = [i for i,x in enumerate(report) if 'deaths,' in x or 'cases,' in x or 'cases_nonlin,' in x]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(range(coldata_adj2.shape[1])) - set(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_clusalign(scountries,coldata_adj2,report,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(len(scountries),coldata_adj2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(coldata_adj[1,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the colours are best understood as hue with value v = intensity related to membership prob\n",
    "# note that unclustered points had probdata values of 0 formerly, now corrected to give outlier_score_\n",
    "#\n",
    "# We should be assigning countries to at least 4 categories : probably five.  Cluster 0,1,2 other cluster and no cluster (-1)\n",
    "# Currently the code relies on the color assignments cluster 0 [1,0,0]  1 [0,1,0]  2 [0,0,1] and only works for 3 clusters.\n",
    "# The unclustered color of [1,1,1] did not disrupt if the probability was always 0 : this will not work with outlier extension\n",
    "# Other clusters higher in number were assigned rather biassedly to one of 0,1,2 : this needs fixing\n",
    "# \n",
    "\n",
    "# count +1 for any RGB component\n",
    "def cscore(crow,cols):\n",
    "    rgbsc = [0.0]*3\n",
    "    for j in cols:\n",
    "        if crow[j][0] >0:\n",
    "            rgbsc[0] = rgbsc[0]+1\n",
    "        if crow[j][1] >0:\n",
    "            rgbsc[1] = rgbsc[1]+1\n",
    "        if crow[j][2] >0:\n",
    "            rgbsc[2] = rgbsc[2]+1\n",
    "    return rgbsc\n",
    "\n",
    "# sum RGB components\n",
    "def cscore_org(crow,cols):\n",
    "    rgbsc = [0.0]*3\n",
    "    for j in cols:\n",
    "        rgbsc[0] = rgbsc[0]+crow[j][0]\n",
    "        rgbsc[1] = rgbsc[1]+crow[j][1]\n",
    "        rgbsc[2] = rgbsc[2]+crow[j][2]   \n",
    "    return rgbsc\n",
    "\n",
    "#sum weighted hues\n",
    "def hscore_org(crow,cols):\n",
    "    hsvmean = color_mean_rgb_to_hsv([crow[j] for j in cols],'all')\n",
    "    return hsvmean\n",
    "\n",
    "\n",
    "def swizzle(countries,data,cols):\n",
    "    rgb = [None]*len(countries)\n",
    "    for i in range(len(countries)):\n",
    "        for j in range(data.shape[1]):\n",
    "            rgbsc = cscore(data[i,:,:],cols)\n",
    "        rgb[i] = np.argmax(rgbsc)\n",
    "    rtn = [None]*len(countries)\n",
    "    cnt = 0\n",
    "    print('-------blue---------')\n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 2:  #blue\n",
    "            rtn[cnt] = i\n",
    "            print(cnt,i,countries[i])\n",
    "            cnt = cnt+1\n",
    "    print('-------green---------')\n",
    "\n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 1:  # green\n",
    "            rtn[cnt] = i\n",
    "            print(cnt,i,countries[i])\n",
    "            cnt = cnt+1    \n",
    "    print('-------red---------')\n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 0:  # red    \n",
    "            rtn[cnt] = i\n",
    "            print(cnt,i,countries[i])\n",
    "            cnt = cnt+1\n",
    "    print('cnt =',cnt)\n",
    "    return rtn\n",
    "\n",
    "def swizzleRGB(countries,data,cols):\n",
    "    rgb = [None]*len(countries)\n",
    "    for i in range(len(countries)):\n",
    "        for j in range(data.shape[1]):\n",
    "            rgbsc = cscore(data[i,:,:],cols)\n",
    "        rgb[i] = np.argmax(rgbsc)\n",
    "    rtn = {}\n",
    "    rtn['R']=[]\n",
    "    rtn['G']=[]\n",
    "    rtn['B']=[]\n",
    "    cnt = 0\n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 2:  #blue\n",
    "            rtn['B'].append(countries[i])\n",
    "            cnt = cnt+1\n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 1:  # green\n",
    "            rtn['G'].append(countries[i])\n",
    "\n",
    "            cnt = cnt+1    \n",
    "    for i in range(len(rgb)):\n",
    "        if rgb[i] == 0:  # red    \n",
    "            rtn['R'].append(countries[i])\n",
    "            cnt = cnt+1\n",
    "    print('cnt =',cnt)\n",
    "    return rtn\n",
    "\n",
    "def swizzle2(countries,data,cols,refcol):\n",
    "    eps = 0.0001\n",
    "    clus = [None]*len(countries)\n",
    "    rgblist = [None]*len(countries)\n",
    "    hsvdic = {}\n",
    "    hsvrefs = [mpcolors.rgb_to_hsv(c) for c in data[:,refcol]]\n",
    "    huesref  = np.sort(list(set([hsv[0] for hsv in hsvrefs if hsv[1] > eps])))\n",
    "    # print('huesref',huesref)\n",
    "    for i in range(len(countries)):\n",
    "        hsvsc = hscore_org(data[i,:,:],cols)\n",
    "        hue = hsvsc[0]\n",
    "        sat = hsvsc[1]\n",
    "        if sat <= 0.5:  # mean is classed as unclustered\n",
    "            clus[i] = -1\n",
    "        else:\n",
    "            clus[i] = closest_hue(hue,huesref)\n",
    "        hsvdic.update({countries[i]:hsvsc})\n",
    "        rgblist[i] = mpcolors.hsv_to_rgb(hsvsc)  \n",
    "    # print('clus',clus,'len',len(clus))\n",
    "    rtn = [None]*len(countries)\n",
    "    cnt = 0\n",
    "    for j in set(clus):\n",
    "        print('-------class',j,'---------')\n",
    "        for i in range(len(countries)):\n",
    "            if clus[i] == j:  \n",
    "                rtn[cnt] = i\n",
    "                # print(cnt,i,countries[i],rgblist[i],hsvlist[i])\n",
    "                print(cnt,i,countries[i])\n",
    "                cnt = cnt+1\n",
    "    print('cnt =',cnt)\n",
    "    return rtn,rgblist,hsvdic\n",
    "\n",
    "def swizzle_class(countries,data,cols,refcol):\n",
    "    clus = [None]*len(countries)\n",
    "    huesref  = np.sort(list(set([mpcolors.rgb_to_hsv(c)[0] for c in data[:,refcol]])))\n",
    "    # print('huesref',huesref)\n",
    "    for i in range(len(countries)):\n",
    "        hsvsc = hscore_org(data[i,:,:],cols)\n",
    "        hue = hsvsc[0]\n",
    "        sat = hsvsc[1]\n",
    "        if sat <= 0.5:  # mean is classed as unclustered\n",
    "            clus[i] = -1\n",
    "        else:\n",
    "            clus[i] = closest_hue(hue,huesref)\n",
    "    rtn = {}\n",
    "    for cl in set(clus):\n",
    "        rtn[cl]=[]\n",
    "    cnt = 0\n",
    "    for j in set(clus):\n",
    "        # print('-------class',j,'---------')\n",
    "        for i in range(len(countries)):\n",
    "            if clus[i] == j:\n",
    "                rtn[j].append(countries[i])\n",
    "                # print(cnt,i,countries[i])\n",
    "                cnt = cnt+1\n",
    "    print('cnt =',cnt)\n",
    "    return rtn\n",
    "\n",
    "def swizzleHSV(countries,data,cols,refcol):\n",
    "    rtn = {}\n",
    "    clus = [None]*len(countries)\n",
    "    huesref  = np.sort(list(set([mpcolors.rgb_to_hsv(c)[0] for c in data[:,refcol]])))\n",
    "    # print('huesref',huesref)\n",
    "    for i in range(len(countries)):\n",
    "        hsvsc = hscore_org(data[i,:,:],cols)\n",
    "        hue = hsvsc[0]\n",
    "        sat = hsvsc[1]\n",
    "        if sat <= 0.5:  # mean is classed as unclustered\n",
    "            clus[i] = -1\n",
    "        else:\n",
    "            clus[i] = closest_hue(hue,huesref)\n",
    "        rtn[countries[i]]=(clus[i],hsvsc[0],hsvsc[1],hsvsc[2])\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx,rgblist,hsvdic = swizzle2(scountries,coldata_adj2,cols,refclustering)\n",
    "#print(cols.idx)\n",
    "dat = np.array([coldata_adj2[i] for i in idx])  # dat is swizzle2 sorted coldata_adj2\n",
    "swcountries = [scountries[i] for i in idx]      # swcountries is swizzle2 sorted scountries\n",
    "plot_clusalign(swcountries,dat,report,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export clusters for world map together with hsv info\n",
    "foo = swizzleHSV(scountries,coldata_adj2,cols,refclustering)\n",
    "with open('clusalign_hsv.pk','wb') as fp:\n",
    "    pk.dump(foo,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export clusters for world map\n",
    "foo = swizzle_class(scountries,coldata_adj2,cols,refclustering)\n",
    "with open('clusalign.pk','wb') as fp:\n",
    "    pk.dump(foo,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(clusdata_all['deaths']['Germany']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in foo:   \n",
    "    print([cc for cc in foo[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quantile plots\n",
    "dtype = 'deaths'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:   \n",
    "    dats = [[max(x/max(clusdata_all[dtype][cc]),0.) for x in clusdata_all[dtype][cc]] for cc in foo[label] ]\n",
    "    dats = np.transpose(np.array(dats))\n",
    "    pdats = [pd.Series(dat) for dat in dats]\n",
    "    qdats = [[pdat.quantile(q) for q in [0.,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.]] for pdat in pdats]\n",
    "    data = np.transpose(np.array(qdats))\n",
    "    # data = qdats\n",
    "    x = range(len(data[0]))\n",
    "    clrs = ['#f0f0f0','#c0c0c0','#505050','#303030','#ff0000','#00ff00','#303030','#505050','#c0c0c0','#f0f0f0'] # clrs[0] not used\n",
    "    for i in range(1,len(data)):\n",
    "        ax[cnt].fill_between(x,data[i-1],data[i],alpha=0.8,color=clrs[i-1]);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quantile plots\n",
    "dtype = 'cases'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:   \n",
    "    dats = [[max(x/max(clusdata_all[dtype][cc]),0.) for x in clusdata_all[dtype][cc]] for cc in foo[label] ]\n",
    "    dats = np.transpose(np.array(dats))\n",
    "    pdats = [pd.Series(dat) for dat in dats]\n",
    "    qdats = [[pdat.quantile(q) for q in [0.,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.]] for pdat in pdats]\n",
    "    data = np.transpose(np.array(qdats))\n",
    "    # data = qdats\n",
    "    x = range(len(data[0]))\n",
    "    clrs = ['#f0f0f0','#c0c0c0','#505050','#303030','#ff0000','#00ff00','#303030','#505050','#c0c0c0','#f0f0f0'] # clrs[0] not used\n",
    "    for i in range(1,len(data)):\n",
    "        ax[cnt].fill_between(x,data[i-1],data[i],alpha=0.8,color=clrs[i-1]);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quantile plots\n",
    "dtype = 'cases_nonlin'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:   \n",
    "    dats = [[max(x/max(clusdata_all[dtype][cc]),0.) for x in clusdata_all[dtype][cc]] for cc in foo[label] ]\n",
    "    dats = np.transpose(np.array(dats))\n",
    "    pdats = [pd.Series(dat) for dat in dats]\n",
    "    qdats = [[pdat.quantile(q) for q in [0.,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.]] for pdat in pdats]\n",
    "    data = np.transpose(np.array(qdats))\n",
    "    # data = qdats\n",
    "    x = range(len(data[0]))\n",
    "    clrs = ['#f0f0f0','#c0c0c0','#505050','#303030','#ff0000','#00ff00','#303030','#505050','#c0c0c0','#f0f0f0'] # clrs[0] not used\n",
    "    for i in range(1,len(data)):\n",
    "        ax[cnt].fill_between(x,data[i-1],data[i],alpha=0.8,color=clrs[i-1]);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype = 'cases'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:\n",
    "    for cc in foo[label]:\n",
    "        rgb = mpcolors.hsv_to_rgb(hsvdic[cc])\n",
    "        mx = max(clusdata_all[dtype][cc])\n",
    "        dat = [max(x/mx,0.) for x in clusdata_all[dtype][cc]]\n",
    "        ax[cnt].plot(dat,alpha=0.8,c=rgb);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype = 'cases_nonlin'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:\n",
    "    for cc in foo[label]:\n",
    "        rgb = mpcolors.hsv_to_rgb(hsvdic[cc])\n",
    "        mx = max(clusdata_all[dtype][cc])\n",
    "        # mx = 1\n",
    "        dat = [max(x/mx,0.) for x in clusdata_all[dtype][cc]]\n",
    "        ax[cnt].plot(dat,alpha=0.8,c=rgb);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype = 'deaths'\n",
    "fig, ax = plt.subplots(1,len(list(set(foo))),figsize=(30,7))\n",
    "cnt = 0\n",
    "for label in foo:\n",
    "    for cc in foo[label]:\n",
    "        rgb = mpcolors.hsv_to_rgb(hsvdic[cc])\n",
    "        mx = max(clusdata_all[dtype][cc])\n",
    "        dat = [max(x/mx,0.) for x in clusdata_all[dtype][cc]]\n",
    "        ax[cnt].plot(dat,alpha=0.8,c=rgb);\n",
    "    ax[cnt].set_title(dtype+' '+('%d' % label))\n",
    "    cnt = cnt+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pops = [np.log10(population_owid[cc][-1]) for cc in scountries]\n",
    "popdensity = [np.log10(population_density_owid[cc][-1]) for cc in scountries]\n",
    "gdp = [np.log10(gdp_per_capita_owid[cc][-1]) for cc in scountries]\n",
    "colors = np.array([mpcolors.hsv_to_rgb(hsvdic[cc]) for cc in scountries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(30,10))\n",
    "axes[0].scatter(pops,popdensity,c=colors)\n",
    "axes[0].set_xlabel('population size(log 10)')\n",
    "axes[0].set_ylabel('population per sq km (log 10)')\n",
    "axes[1].scatter(pops,gdp,c=colors)\n",
    "axes[1].set_xlabel('population size (log 10)')\n",
    "axes[1].set_ylabel('gdp per capita (log 10)')\n",
    "axes[2].scatter(popdensity,gdp,c=colors)\n",
    "axes[2].set_xlabel('population per sq km (log 10)')\n",
    "axes[2].set_ylabel('gdp per capita (log 10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gdp_per_capita_owid = get_data_owid(owid_file,datatype='gdp_per_capita',dataaccum = 'daily')\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pops,popdensity,gdp,c=colors)\n",
    "ax.set_xlabel('population (log 10)')\n",
    "ax.set_ylabel('population per sq km (log 10)')\n",
    "ax.set_zlabel('gdp per capita (log 10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('score\\t\\tcase\\tminc\\tdim\\tnclus\\tunclus\\tvaliditysc')\n",
    "[x for x in report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('score\\t\\tcase\\tminc\\tdim\\tnclus\\tunclus\\tvaliditysc')\n",
    "[x for x in report if 'scaled' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('score\\t\\tcase\\tminc\\tdim\\tnclus\\tunclus\\tvaliditysc')\n",
    "[x for x in report if 'score1' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('score\\t\\tcase\\tminc\\tdim\\tnclus\\tunclus\\tvaliditysc')\n",
    "[x for x in report if 'score2' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(x,info[x]['dim'],info[x]['unclustered']) for x in cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minscore1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical analysis of an optimal clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rerun for optimal parameters\n",
    "# minscore1 : could use maxvalid or minscore1 instead\n",
    "(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2) = minscore1[0][0]\n",
    "\n",
    "food = ClusterFit(clusdata_all['cases_nonlin'])\n",
    "dat = food.dat\n",
    "dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "\n",
    "fpca_disc = FPCA(n_components=ncomp)\n",
    "fpca_disc.fit(dat_disc)\n",
    "foo = fpca_disc.transform(dat_disc)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples,gen_min_span_tree=True)\n",
    "labels = clusterer.fit_predict(foo)\n",
    "try:\n",
    "    validity = hdbscan.validity.validity_index(foo, labels)\n",
    "    print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "          len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "          sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',np.round(validity,5))\n",
    "except:\n",
    "    validity=None\n",
    "\n",
    "\n",
    "    print('hdbscan_min_clus=',minc,':  ',ncomp ,'FPCAcomponents:  ',\n",
    "      len(set([x for x in labels if x>-1])),'clusters;  ',\n",
    "      sum([1 for x in labels if x>-1]),'clustered;  ',sum([1 for x in labels if x==-1]),'unclustered; ','validity =',validity)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis', \n",
    "                                      edge_alpha=0.6, \n",
    "                                      node_size=80, \n",
    "                                      edge_linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clusterer.condensed_tree_.plot()\n",
    "clusterer.condensed_tree_.plot(select_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projection = TSNE().fit_transform(dat)\n",
    "color_palette = sns.color_palette('Paired', 12)\n",
    "cluster_colors = [color_palette[x] if x >= 0\n",
    "else (0.5, 0.5, 0.5)\n",
    "for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer.probabilities_)]\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "plt.scatter(*projection.T, s=200, linewidth=2, c=cluster_member_colors, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(*projection.T,c=cluster_member_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(probdata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd method using basis:\n",
    "\n",
    "basis = skfda.representation.basis.BSpline(n_basis=20)\n",
    "basis_dat_disc = dat_disc.to_basis(basis)\n",
    "basis_dat_disc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rescale(v,d):\n",
    "    \"\"\" functional form of correction factor using simple inversion formula\n",
    "        for with v2'=1/(1-v2) the dimensionality correction v = v2 * v2'/(v2'+d/2-1)\n",
    "        projecting equivalent validity at dim = 2\"\"\"\n",
    "    if d > 12.:\n",
    "        d = 12.\n",
    "    logd = np.log(d/2.)\n",
    "    return v*(1.+logd)/(1.+v*logd)\n",
    "\n",
    "maxvalid = [None,None,None,None,None,None]\n",
    "maxvalidval= 0.0\n",
    "maxvalidsc = [None,None,None,None,None,None]\n",
    "maxvalidscval= 0.0\n",
    "minscore1 = [None,None,None,None,None,None]\n",
    "minscore1val = 999.\n",
    "minscore2 = [None,None,None,None,None,None]\n",
    "minscore2val = 999.\n",
    "report = [' ']*4*6\n",
    "reportdata = [None]*4*6\n",
    "probdata=np.zeros((4*6,len(dat)),dtype=float)\n",
    "outlierdata=np.zeros((4*6,len(dat)),dtype=float)\n",
    "clusdata = np.zeros((4*6,len(countries)),dtype=np.integer)\n",
    "info =  pd.DataFrame(columns=['type','minc','mins','ncomp','clustered','unclustered','validity','validitysc','score1','score2'])\n",
    "infomax =  pd.DataFrame(columns=['type','minc','mins','ncomp','clustered','unclustered','validity','validitysc','score1','score2'])\n",
    "cnt=0\n",
    "for ic,case in enumerate(cases):\n",
    "    foocase = ClusterFit(clusdata_all[case])\n",
    "    dat = foocase.dat\n",
    "    dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "\n",
    "    print('--------------------------',case,'-------------------------------')\n",
    "    maxvalidval= 0.0\n",
    "    maxvalidscval= 0.0\n",
    "    minscore1val = 999.\n",
    "    minscore2val = 999.\n",
    "    for ncomp in range(2,16):  # code will only work if reference value 2 included in range\n",
    "        min_samples = 2\n",
    "        for minc in range(3,10):\n",
    "            fpca_disc = FPCA(n_components=ncomp)\n",
    "            fpca_disc.fit(dat_disc)\n",
    "            foo = fpca_disc.transform(dat_disc)\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples)\n",
    "            labels = clusterer.fit_predict(foo)\n",
    "            nclus = len(set([x for x in labels if x>-1]))\n",
    "            nclustered = sum([1 for x in labels if x>-1])\n",
    "            nunclustered = sum([1 for x in labels if x==-1])\n",
    "            try:\n",
    "                validity = hdbscan.validity.validity_index(foo, labels)\n",
    "                validity = max(validity,0.001)\n",
    "                validitysc = rescale(validity,ncomp) \n",
    "                score1 = 1.0/validitysc + nunclustered/5 + np.abs(nclus-4)/2\n",
    "                score2 = nunclustered*(4.+np.abs(nclus-4))/(validitysc*20)\n",
    "                if validity > maxvalidval:\n",
    "                    maxvalidval = validity\n",
    "                    maxvalid[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4,:] = labels[:]\n",
    "                    report[ic*4] = 'max normal validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if validitysc > maxvalidscval:\n",
    "                    maxvalidscval = validitysc\n",
    "                    maxvalidsc[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+1,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+1,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+1,:] = labels[:]\n",
    "                    report[ic*4+1] = 'max scaled validity: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+1] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if score1 <  minscore1val:\n",
    "                    minscore1val = score1\n",
    "                    minscore1[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]   \n",
    "                    probdata[ic*4+2,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+2,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+2,:] = labels[:]\n",
    "                    report[ic*4+2] = 'min combined score1: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+2] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                if score2 <  minscore2val:\n",
    "                    minscore2val = score2\n",
    "                    minscore2[ic] = [(minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2)]\n",
    "                    probdata[ic*4+3,:] = clusterer.probabilities_[:]\n",
    "                    outlierdata[ic*4+3,:] = clusterer.outlier_scores_[:]\n",
    "                    clusdata[ic*4+3,:] = labels[:]\n",
    "                    report[ic*4+3] = 'min combined score2: %15s,%2d,%3d,%3d,%3d,%5.2f' % (case,minc,ncomp,nclus,nunclustered,validitysc)\n",
    "                    reportdata[ic*4+3] = (case,minc,ncomp,nclus,nunclustered,validity,validitysc,score1,score2)\n",
    "                    \n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                      nclus,'clusters;  ',\n",
    "                      nclustered,'clustered;  ',\n",
    "                      nunclustered,'unclustered; ','validity =',np.round(validity,5),'validitysc =',np.round(validitysc,5),\n",
    "                      'score1:',np.round(score1,3),'score2:',np.round(score2,3))\n",
    "            except:\n",
    "                validity=None\n",
    "                print('hdbscan: ',minc,'minc:  ',min_samples,'mins:  ',ncomp ,'FPCAcomponents:  ',\n",
    "                  nclus,'clusters;  ',\n",
    "                  nclustered,'clustered;  ',nunclustered,'unclustered; ','validity =',validity)\n",
    "            info.loc[cnt] = [case,minc,min_samples,ncomp,nclustered,nunclustered,validity,validitysc,score1,score2]\n",
    "            cnt = cnt+1\n",
    "\n",
    "        print('--------------------------')\n",
    "    print('---------------------------------------------------------')\n",
    "    print('minc,min_samples,ncomp,nclus,nclustered,nunclustered,validity,validitysc,score1,score2')\n",
    "    print('maxvalid ',maxvalid[ic])\n",
    "    print('maxvalidsc ',maxvalidsc[ic])\n",
    "    print('minscore1',minscore1[ic])\n",
    "    print('minscore2',minscore2[ic])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case = 'deaths'\n",
    "ncomp = 11\n",
    "minc = 3\n",
    "min_samples = 2\n",
    "foocase = ClusterFit(clusdata_all[case])\n",
    "dat = foocase.dat\n",
    "dat_disc = skfda.representation.grid.FDataGrid(dat,list(range(len(dat[0]))))\n",
    "\n",
    "fpca_disc = FPCA(n_components=ncomp)\n",
    "fpca_disc.fit(dat_disc)\n",
    "foo = fpca_disc.transform(dat_disc)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples).fit(foo)\n",
    "labels = clusterer.fit_predict(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=minc,min_samples=min_samples,prediction_data=True).fit(foo)\n",
    "soft = hdbscan.all_points_membership_vectors(clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=0\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,4))\n",
    "for idx in [0,1,2]:\n",
    "    for _ in range(len(soft[0])):\n",
    "        datc = [cc[idx] for cc in soft]\n",
    "        ax[idx].hist(datc,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = Axes3D(fig)\n",
    "x=[ss[0] for ss in soft]\n",
    "y = [ss[1] for ss in soft]\n",
    "z = [ss[2] for ss in soft]\n",
    "ax.scatter(x,y,z,c=labels)\n",
    "tmp = 0.5\n",
    "x = [tmp,0,0]\n",
    "y = [0,tmp,0]\n",
    "z = [0,0,tmp]\n",
    "verts = [list(zip(x,y,z))]\n",
    "tri = Poly3DCollection(verts,alpha=0.5)\n",
    "tri.set_color('darkred')\n",
    "ax.add_collection3d(tri)\n",
    "ax.plot([0,1],[0,1],[0,1],color='darkred',alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix density plot :\n",
    "Columns : different clusterings.\n",
    "Rows: the countries.\n",
    "\n",
    "1.\tChoose candidate best clustering: this determines labels of clusters\n",
    "2.\tOrder countries by this clustering then alphabetically within cluster\n",
    "3.\tFor this first clustering complete column with prob of country assignment being correct. Assign one of 3-6 colours to each cluster label. Shade colour intensity with prob. e.g. colours (red, green ,blue, yellow, magenta, cyan)\n",
    "4.\tFor next clustering: firstly determine best matches of cluster index to first clustering. If more clusters than best (or for later steps those already registered), add new clusters to cluster list with new colours. Stop at 6 clusters in registered list. Members of further clusters all receive colour black with grey tone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# redo all clustering compactly \n",
    "# Norman I guess you need to substitute this with dbhscan\n",
    "datasets = [c for c in clusdata_all]\n",
    "print(datasets)\n",
    "res = {}\n",
    "for d in datasets:\n",
    "    print('doing ',d,'...')\n",
    "    mfit = ClusterFit(clusdata_all[d])\n",
    "    # mfit = ClusterFit(clusdata_all[d],fft='fft') # only 2 clusters for 'deaths'\n",
    "    mfit.umap_best_cluster(Nclus=3)\n",
    "    res[d] = mfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_argsort = np.lexsort((countries,res['deaths'].clus_labels))\n",
    "scountries = [countries[clus_argsort[i]] for i in range(len(countries))]\n",
    "probdata2 = np.zeros((len(res),len(countries)),dtype=np.float)\n",
    "clusdata2 = np.zeros((len(res),len(countries)),dtype=np.integer)\n",
    "probdata2s = np.zeros((len(res),len(countries)),dtype=np.float)\n",
    "clusdata2s = np.zeros((len(res),len(countries)),dtype=np.integer)\n",
    "for i,rr in enumerate(res):\n",
    "    foo = res[rr]\n",
    "    probdata2[i,:] = foo.clus_probs[:]\n",
    "    clusdata2[i,:] = foo.clus_labels[:]\n",
    "    for j in range(len(scountries)):\n",
    "        probdata2s[i,j] = foo.clus_probs[clus_argsort[j]]\n",
    "        clusdata2s[i,j] = foo.clus_labels[clus_argsort[j]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdata = np.random.random((10,10))              # prob of correct assignment to chosen cluster\n",
    "rawdata = np.transpose(probdata2s)\n",
    "# cindex = np.random.random_integers(0,3,(10,10))  # cluster index \n",
    "cindex = np.transpose(clusdata2s)\n",
    "colors = np.array([[1,1,1],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1]]) # black,red,green,blue,yellow,cyan,magenta\n",
    "cluscols = np.transpose(colors[1+cindex[:,:]],(2,0,1)) # transpose to allow elementwise multiplication with rawdata with separate r,g,b\n",
    "coldata = np.transpose(cluscols*rawdata,(1,2,0))   # transpose back to have colours as elements of 2D array\n",
    "report2 = ['umap_pca: %16s' % r for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,15))\n",
    "img = ax.imshow(coldata)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(countries)\n",
    "ax.set_xticks(range(len(res)))\n",
    "ax.set_xticklabels(report2,rotation='vertical')\n",
    "# fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN and PCA clusterings together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusdata12 = np.concatenate((clusdata,clusdata2))\n",
    "probdata12 = np.concatenate((probdata,probdata2))\n",
    "report12 = report + report2\n",
    "clus_argsort = np.lexsort((countries,clusdata12[0]))\n",
    "scountries = [countries[clus_argsort[i]] for i in range(len(countries))]\n",
    "probdata_s = probdata12.copy()\n",
    "clusdata_s = clusdata12.copy()\n",
    "for i in range(len(probdata12)):\n",
    "    foo = probdata12[i]\n",
    "    for j in range(len(scountries)):\n",
    "        probdata_s[i,j] = probdata12[i,clus_argsort[j]]\n",
    "        clusdata_s[i,j] = clusdata12[i,clus_argsort[j]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdata = np.random.random((10,10))              # prob of correct assignment to chosen cluster\n",
    "rawdata = np.transpose(probdata_s)\n",
    "# cindex = np.random.random_integers(0,3,(10,10))  # cluster index \n",
    "cindex = np.transpose(clusdata_s)\n",
    "colors = np.array([[1,1,1],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1]]) # black,red,green,blue,yellow,cyan,magenta\n",
    "cluscols = np.transpose(colors[cindex[:,:]+1],(2,0,1)) # transpose to allow elementwise multiplication with rawdata with separate r,g,b\n",
    "coldata = np.transpose((cluscols+3*cluscols*rawdata)/4.,(1,2,0))   # transpose back to have colours as elements of 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,20))\n",
    "img = ax.imshow(coldata)\n",
    "ax.set_yticks(range(len(countries)))\n",
    "ax.set_yticklabels(scountries)\n",
    "ax.set_xticks(range(len(clusdata_s)))\n",
    "plt.setp(ax.get_xticklabels(), rotation='vertical', family='monospace')\n",
    "ax.set_xticklabels(report12,rotation='vertical')\n",
    "# fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping between clusters, so that recolor columns to best match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_int(a,b):\n",
    "    if len(set(a)) > 0 or len(set(b)) > 0:\n",
    "        return len(set(a).intersection(set(b)))\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def matchset(a,x):\n",
    "    rtn = [i for i in range(len(a)) if a[i] == x]\n",
    "    return rtn\n",
    " \n",
    "from matplotlib import colors as mpcolors\n",
    "\n",
    "def clust(clustering_a,clustering_b,colors_a,colors_b): \n",
    "    \"\"\" relables clustering b to match clustering a\n",
    "        if more than one cluster in a optimally matches a particular cluster in b, then color of b is merger of colors in a\n",
    "        if more than one cluster in b optimally matches a particular cluster in a, then colors in a merged and split for b\n",
    "    \"\"\"\n",
    "    labels_a = set(clustering_a)\n",
    "    labels_b = set(clustering_b)\n",
    "    \n",
    "    if len(labels_a) != len(colors_a): print('error wrong color list length for a')\n",
    "    if len(labels_b) != len(colors_b): print('error wrong color list length for b')\n",
    "            \n",
    "    a_to_b = {}\n",
    "    b_to_a = {}\n",
    "    a_cols = {a : colors_a[i] for i,a in enumerate(labels_a)}\n",
    "    b_cols = {b : colors_b[i] for i,b in enumerate(labels_b)}\n",
    "    \n",
    "    for a in labels_a:\n",
    "        maxscore = 0\n",
    "        maxlab = -2\n",
    "        for b in labels_b:\n",
    "            score = score_int(matchset(clustering_a,a),matchset(clustering_b,b))\n",
    "            if score > maxscore:\n",
    "                maxscore = score\n",
    "                maxlab = b\n",
    "        a_to_b.update({a:maxlab})\n",
    "\n",
    "    for b in labels_b:\n",
    "        maxscore = 0\n",
    "        maxlab = -2\n",
    "        for a in labels_a:\n",
    "            score = score_int(matchset(clustering_a,a),matchset(clustering_b,b))\n",
    "            if score > maxscore:\n",
    "                maxscore = score\n",
    "                maxlab = a\n",
    "        b_to_a.update({b:maxlab})\n",
    "    \n",
    "    for b in labels_b:   # first adjust colors in b to match mapped clusters from a (transfer and merge)\n",
    "        amap = [a for a in labels_a if a_to_b[a] == b]\n",
    "        if len(amap) > 0:\n",
    "            h = sum([mpcolors.rgb_to_hsv(a_cols[a])[0] for a in amap])/len(amap) # average hue from amap\n",
    "            s = mpcolors.rgb_to_hsv(b_cols[b])[1] # take s saturation from b\n",
    "            v = mpcolors.rgb_to_hsv(b_cols[b])[2] # take v from b\n",
    "            b_cols[b] = mpcolors.hsv_to_rgb([h,s,v]) # back to rgb\n",
    "\n",
    "    for a in labels_a:   # now readjust colors in b that both map to same a (split)\n",
    "        bmap = [b for b in labels_b if b_to_a[b] == a]\n",
    "        if len(bmap)>1:\n",
    "            h = sum([mpcolors.rgb_to_hsv(b_cols[b])[0] for b in bmap])/len(bmap) # average hue from bmap  \n",
    "            ha = mpcolors.rgb_to_hsv(a_cols[a])[0]\n",
    "            hb = np.linspace(abs(h-ha/4.),abs(h+ha/4.),len(bmap))\n",
    "            #print('hb[',hb[0],hb[1],']',h,ha,ha/4.,abs(h-ha/4.),abs(h+ha/4.))\n",
    "            for i,b in enumerate(bmap):\n",
    "                s = mpcolors.rgb_to_hsv(b_cols[b])[1] # take s saturation from b\n",
    "                #print('s',s)\n",
    "                v = mpcolors.rgb_to_hsv(b_cols[b])[2] # take v from b\n",
    "                #print('v',v)\n",
    "                b_cols[b]= mpcolors.hsv_to_rgb([hb[i],s,v])\n",
    "                #print('hb[i],b_cols[b]',hb[i],b_cols[b])\n",
    "    return b_cols,a_to_b,b_to_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust([0,1,1,0,2,2,2,0],[1,0,0,1,2,2,2,1],[[1,0,0],[0,1,0],[0,0,1]],[[1,0,0],[0,1,0],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust([0,1,1,0,2,2,2,0],[1,0,0,1,2,3,3,1],[[1,0,0],[0,1,0],[0,0,1]],[[1,0,0],[0,1,0],[0,0,1],[1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corcl(a,b):\n",
    "    if len(set(a)) > 0 or len(set(b)) > 0:\n",
    "        return len(set(a).intersection(set(b)))/float(len(set(a).union(set(b))))\n",
    "    else:\n",
    "        return 1 \n",
    "\n",
    "def match1(a,x):\n",
    "    rtn = [1 if a[i] == x else 0 for i in range(len(a)) ]\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family matching paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* [the paper](https://hal.inria.fr/hal-01514872/document)\n",
    "* [the user manual](https://sbl.inria.fr/doc/D_family_matching-user-manual.html)\n",
    "* [Structural Bioinformatics Library](https://sbl.inria.fr/doc/index.html) C++/python\n",
    "* [jupyter notebook](https://sbl.inria.fr/demos/D_family_matching.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "341px",
    "width": "225px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.0625px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
